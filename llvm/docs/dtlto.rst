Distributed ThinLTO (DTLTO)
===========================

DTLTO allows for the distribution of backend ThinLTO compilations via external
distribution systems, e.g. FastBUILD. It is currently possible to distribute
ThinLTO compilations by using separate thin-link, backend compilation, and link
steps. This is coordinated through a build system such as Bazel. However, this
often requires changes to the user's build process. DTLTO distribution is managed
internally in LLD as part of the traditional link step and therefore should be
usable via any build that can support in-process ThinLTO.

Initial RFC: https://discourse.llvm.org/t/rfc-integrated-distributed-thinlto/69641


Overview of operation
---------------------

For any backend compilation that is not available from the ThinLTO cache, LLD will create:

- The required summary index shards.
- A list of required input and output files.
- The -cc1-level command line to perform the backend compilation.

This backend compilation information will be supplied to a distributor which will block performing the backend compilations via a distribution system (e.g., FastBUILD). Once the distributor's execution is complete LLD will insert the compiled native object files into the link to allow it to complete. Any temporary files, such as the summary index shards, will be cleaned up so that the set of files remaining matches what would have been produced via the normal in-process ThinLTO with the same inputs.


How distributors are invoked
----------------------------

Clang and LLD support options to allow the user to specify a process or script that distributes backend compilations (distributor).

Options can be passed to the distributor from the front end via CLI options, this is transparent to the toolchain.

At this stage, the backend compilations are performed by `clang -cc1`. The process that performs the backend compilations must implement Clang's -cc1-level interface.

TODO: Link to LLD and Clang CLI documents. 

When performing ThinLTO and invoked with a distributor, LLD will create a JSON job description file (describing the backend compilations to perform) and then invoke the distributor with that job file. The job file describes:

1. The command line to invoke to perform compilations.
2. The set of input files required.
3. The set of output files required.

Input files might be, bitcode files, summary index files, profile data, etc...
Output files might be compiled object files, toolchain metrics files, etc..

The JSON schema used is available here: llvm/utils/dtlto/dtlto_schema.json.

Schema features:
- Uses indexing into array fields this avoids repeating strings and it allows the distributor to understand what the inputs and outputs are, so that it can rewrite the command lines if it needs to change their names - due to constraints of the underlying distribution system. Contrived example: A system may only be able to read and write remote files to C:\sandbox. The remote command can be adjusted by the distributor to do that. Once the outputs are back on the local system the distributor can rename them as required.
- Allows for the identifier for a bitcode module to differ from its on disk filename. This feature is to allow archive members to be supported in the future (see the later section on archive support).
- Versioned to allow for future modification.

Constraints:
- Matching versions of clang and lld should be used.
- The distributor used must support the JSON schema generated by the version of LLD in use.

Example JSON file:

The below JSON is output for the backend compilation for three modules:
- libmain.a(main.o)
- "c.o"
- liba.a(a.o)

{
    "version": {
        "major": 0,
        "minor": 0
    },
    "common": {
        "args": [
            "-cc1",
            "-O2",
            "-fdiagnostics-format",
            "msvc",
            "-faddrsig",
            "-ffunction-sections",
            "-fdata-sections",
            "-fbasic-block-sections=none",
            "-mrelocation-model",
            "pic",
            "-fprofile-instrument-use-path=my.profdata",
            "-o",
            [
                "",
                "outputs",
                0
            ],
            "-emit-obj",
            "-x",
            "ir",
            [
                "",
                "inputs",
                0
            ],
            [
                "-fthinlto-index=",
                "inputs",
                1
            ],
            "-triple",
            "x86_64-sie-ps5"
        ],
        "codegen-tool": "/usr/bin/local/clang.exe",
        "inputs": [
            "my.profdata"
        ],
        "modules": [
            [
                "libmain.a(main.o at 82)",
                "main.<uid>.o"
            ],
            "c.o",
            [
                "liba.a(a.o at 78)",
                "a.o"
            ]
        ]
    },
    "jobs": [
        {
            "inputs": [
                0,
                "main.<uid>.o.native.o.thinlto.bc",
                1,
                2
            ],
            "outputs": [
                "main.<uid>.o.native.o"
            ]
        },
        {
            "inputs": [
                2,
                "a.<uid>.o.native.o.thinlto.bc"
            ],
            "outputs": [
                "a.<uid>.o.native.o"
            ]
        },
        {
            "inputs": [
                1,
                "c.<uid>.native.o.thinlto.bc"
            ],
            "outputs": [
                "c.<uid>.native.o"
            ]
        }
    ]
}

To create the backend compilation command for liba.a(a.o) first the inputs for that modules are expanded:

...
        {
            "inputs": [
                "libmain.a(main.o at 82)",
                "main.<uid>.o.native.o.thinlto.bc",
                "c.o",
                "liba.a(a.o at 78)"
            ],
            "outputs": [
                "main.<uid>.o.native.o"
            ]
        },
...

The command line template is expanded:
/usr/bin/local/clang.exe -cc1 -O2 -fdiagnostics-format msvc -faddrsig -ffunction-sections -fdata-sections -fbasic-block-sections=none -mrelocation-model pic -fprofile-instrument-use-path=my.profdata -o" "main.<uid>.o.native.o -emit-obj -x ir libmain.a(main.o at 82) -fthinlto-index=main.<uid>.o.native.o.thinlto.bc -triple x86_64-sie-ps5

The input files and output files are recorded:
inputs = "my.profdata", "libmain.a(main.o at 82)", "main.<uid>.o.native.o.thinlto.bc", "c.o", "liba.a(a.o at 78)"
outputs = "main.<uid>.o.native.o"

The module mapping is recorded:
"libmain.a(main.o at 82)" => "main.<uid>.o", "liba.a(a.o at 78)" => "a.o"

Note: The module mapping is not yet consumed by Clang. However, we propose it as a mechanism to support modules from archives. Please see the later section on archive support for more.


Distributors
------------

To write a distributor, a program must be created that consumes the JSON job description files and translates them into requests to perform the backend compilations on the target distribution system (e.g., FastBUILD). The distributor must
block until the backend compilations are complete. A non-zero error code must be returned in the case that compilation fails. Any output from the distributor will be displayed. Distributors can be written in scripting languages (such as Python). The script file can either be made executable (using OS-specific mechanisms), or an interpreter can be specified as the distributor process and the script can be supplied to the interpreter using the appropriate Clang or LLD CLI option to forward arguments to the distributor process.

Distributors are expected to be maintained out-of-tree as part of a distribution system such as FastBUILD. Example scripts demonstrating the principles but executing the backend compilations in a multiprocess manner on the local machine (rather than via a build system) will be stored in-tree in llvm/utils/dtlto.

Our team has a particular interest in SN-DBS (https://sonyinteractive.com/en/press-releases/2007/sony-computer-entertainment-boosts-playstation3-game-development-environment/). We will be maintaining a distributor that ships with SN-DBS and is tested with the supported LLVM toolchains. Integration testing will be maintained internally to ensure that game code builds and runs correctly when using DTLTO with DBS on supported toolchains.


Temporary files
---------------

The following temporary files may be generated by LLD:

- Summary index shards.
- Unpacked archive modules.
- JSON job description files.

These temporary files will be deleted after the LTO step. Deletion can be time-consuming.

TODO: Add note about the optimization where we create the native object files in the cache directory to avoid costly renaming across disk volumes on Windows... or remove this code for the initial upstream commit?


Archives
--------

Supporting bitcode modules in archives is considered important for DTLTO as the goal of this feature is to allow users to distribute ThinLTO backend compilations with minimal disruption to their existing builds.

With archive modules the ModuleID may not be the path to the file on disk. We intend to implement the following mechanism to support archives.

1. Any archive modules required for LTO will be unpacked to temporary locations on disk.
2. A mapping will be supplied to the compiler for the backend compilations, which maps between the ModuleID and the path to the module on disk. Supplying a mapping is already supported for use by the Gold linker, we can take advantage of this mechanism to implement this feature.

To aid performance a persistent content-addressable storage (https://en.wikipedia.org/wiki/Content-addressable_storage) cache can be implemented so that unmodified archive modules are fetched from a cache if they have been unpacked previously.

Note that we will implement support for archives incrementally. We envision the implementation occurring in the following stages:
- Initial commit - no support for archives.
- Thin archive support implemented, archives are unsupported.
- Archive support added via unpacking, but the unpacking is not cached.
- Caching of archive unpacking supported.


Alternative designs considered
------------------------------

A compiler wrapper, or equivalently an implementation that drives Bazel-style distribution of ThinLTO in the compiler frontend, was considered as an alternative idea. However, this would make it awkward to handle archives.

A plugin mechanism for the linker was considered, where the plugins would provide the support for a given distribution system. However, this was considered to be too disruptive to LLD while not offering a conclusive benefit over calling out to an external process to handle distribution.


Ideas for future improvements
-----------------------------

A possible future enhancement would be to support arbitrary remote opt tools via either:
  - Supplying a tool definition to LLD which would allow for driving the CLI of the opaque tool. For example this could specify how to translate LTO configuration settings into CLI options for that tool.
  - Support for a standard [de]serialization format for LTO configurations that any remote opt tool could implement support for.

Other future improvements (TODO flesh out these ideas):
- Batching
- Launching backend compilations as soon as the information is available.
- Deferred clean-up of temporary files.
